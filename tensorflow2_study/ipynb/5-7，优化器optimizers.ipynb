{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"13622F140BD745CB89EE4DB36C46DBBC","mdEditEnable":false},"source":"# 5-7,优化器optimizers\n\n机器学习界有一群炼丹师，他们每天的日常是：\n\n拿来药材（数据），架起八卦炉（模型），点着六味真火（优化算法），就摇着蒲扇等着丹药出炉了。\n\n不过，当过厨子的都知道，同样的食材，同样的菜谱，但火候不一样了，这出来的口味可是千差万别。火小了夹生，火大了易糊，火不匀则半生半糊。\n\n机器学习也是一样，模型优化算法的选择直接关系到最终模型的性能。有时候效果不好，未必是特征的问题或者模型设计的问题，很可能就是优化算法的问题。\n\n深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\n\n详见《一个框架看懂优化算法之异同 SGD/AdaGrad/Adam》\n\nhttps://zhuanlan.zhihu.com/p/32230623\n\n对于一般新手炼丹师，优化器直接使用Adam，并使用其默认参数就OK了。\n\n一些爱写论文的炼丹师由于追求评估指标效果，可能会偏爱前期使用Adam优化器快速下降，后期使用SGD并精调优化器参数得到更好的结果。\n\n此外目前也有一些前沿的优化算法，据称效果比Adam更好，例如LazyAdam, Look-ahead, RAdam, Ranger等.\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E0420B94A5F945D28A7934B4FDD1D62D","mdEditEnable":false},"source":"## 一，优化器的使用"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"670234E7EBE74425990DE34F221D4E34"},"source":"优化器主要使用apply_gradients方法传入变量和对应梯度从而来对给定变量进行迭代，或者直接使用minimize方法对目标函数进行迭代优化。\n\n当然，更常见的使用是在编译时将优化器传入keras的Model,通过调用model.fit实现对Loss的的迭代优化。\n\n初始化优化器时会创建一个变量optimier.iterations用于记录迭代的次数。因此优化器和tf.Variable一样，一般需要在@tf.function外创建。"},{"cell_type":"code","execution_count":1,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EF4C59C1CD0C474F8E30E159C2588000","collapsed":false,"scrolled":false},"outputs":[],"source":"import tensorflow as tf\nimport numpy as np \n\n#打印时间分割线\n@tf.function\ndef printbar():\n    ts = tf.timestamp()\n    today_ts = ts%(24*60*60)\n\n    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n    minite = tf.cast((today_ts%3600)//60,tf.int32)\n    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n    \n    def timeformat(m):\n        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n            return(tf.strings.format(\"0{}\",m))\n        else:\n            return(tf.strings.format(\"{}\",m))\n    \n    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n                timeformat(second)],separator = \":\")\n    tf.print(\"==========\"*8,end = \"\")\n    tf.print(timestring)\n    "},{"cell_type":"code","execution_count":2,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6BCD23A2B7D74A5495ADC19D5668AE7F","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================15:13:15\nstep =  100\nx =  0.867380381\n\n================================================================================15:13:15\nstep =  200\nx =  0.98241204\n\n================================================================================15:13:15\nstep =  300\nx =  0.997667611\n\n================================================================================15:13:15\nstep =  400\nx =  0.999690711\n\n================================================================================15:13:15\nstep =  500\nx =  0.999959\n\n================================================================================15:13:15\nstep =  600\nx =  0.999994516\n\ny = 0\nx = 0.999995232\n","name":"stdout"}],"source":"# 求f(x) = a*x**2 + b*x + c的最小值\n\n# 使用optimizer.apply_gradients\n\nx = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n\n@tf.function\ndef minimizef():\n    a = tf.constant(1.0)\n    b = tf.constant(-2.0)\n    c = tf.constant(1.0)\n    \n    while tf.constant(True): \n        with tf.GradientTape() as tape:\n            y = a*tf.pow(x,2) + b*x + c\n        dy_dx = tape.gradient(y,x)\n        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n        \n        #迭代终止条件\n        if tf.abs(dy_dx)<tf.constant(0.00001):\n            break\n            \n        if tf.math.mod(optimizer.iterations,100)==0:\n            printbar()\n            tf.print(\"step = \",optimizer.iterations)\n            tf.print(\"x = \", x)\n            tf.print(\"\")\n                \n    y = a*tf.pow(x,2) + b*x + c\n    return y\n\ntf.print(\"y =\",minimizef())\ntf.print(\"x =\",x)"},{"cell_type":"code","execution_count":3,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8CDEB74F31BF4CD79781EF2BB7FE96F5","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch =  1000\ny =  0\nx =  0.999998569\n","name":"stdout"}],"source":"# 求f(x) = a*x**2 + b*x + c的最小值\n\n# 使用optimizer.minimize\n\nx = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n\ndef f():   \n    a = tf.constant(1.0)\n    b = tf.constant(-2.0)\n    c = tf.constant(1.0)\n    y = a*tf.pow(x,2)+b*x+c\n    return(y)\n\n@tf.function\ndef train(epoch = 1000):  \n    for _ in tf.range(epoch):  \n        optimizer.minimize(f,[x])\n    tf.print(\"epoch = \",optimizer.iterations)\n    return(f())\n\ntrain(1000)\ntf.print(\"y = \",f())\ntf.print(\"x = \",x)\n"},{"cell_type":"code","execution_count":4,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9B454C127E3A4A6A80B80D1E957CA448","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Model: \"fake_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nTotal params: 1\nTrainable params: 1\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 100 samples\nEpoch 1/10\n100/100 [==============================] - 0s 2ms/sample - loss: 0.2481\nEpoch 2/10\n100/100 [==============================] - 0s 691us/sample - loss: 0.0044\nEpoch 3/10\n100/100 [==============================] - 0s 631us/sample - loss: 7.6740e-05\nEpoch 4/10\n100/100 [==============================] - 0s 694us/sample - loss: 1.3500e-06\nEpoch 5/10\n100/100 [==============================] - 0s 685us/sample - loss: 1.8477e-08\nEpoch 6/10\n100/100 [==============================] - 0s 622us/sample - loss: 0.0000e+00\nEpoch 7/10\n100/100 [==============================] - 0s 687us/sample - loss: 0.0000e+00\nEpoch 8/10\n100/100 [==============================] - 0s 684us/sample - loss: 0.0000e+00\nEpoch 9/10\n100/100 [==============================] - 0s 636us/sample - loss: 0.0000e+00\nEpoch 10/10\n100/100 [==============================] - 0s 710us/sample - loss: 0.0000e+00\n","name":"stdout"}],"source":"# 求f(x) = a*x**2 + b*x + c的最小值\n# 使用model.fit\n\ntf.keras.backend.clear_session()\n\nclass FakeModel(tf.keras.models.Model):\n    def __init__(self,a,b,c):\n        super(FakeModel,self).__init__()\n        self.a = a\n        self.b = b\n        self.c = c\n    \n    def build(self):\n        self.x = tf.Variable(0.0,name = \"x\")\n        self.built = True\n    \n    def call(self,features):\n        loss  = self.a*(self.x)**2+self.b*(self.x)+self.c\n        return(tf.ones_like(features)*loss)\n    \ndef myloss(y_true,y_pred):\n    return tf.reduce_mean(y_pred)\n\nmodel = FakeModel(tf.constant(1.0),tf.constant(-2.0),tf.constant(1.0))\n\nmodel.build()\nmodel.summary()\n\nmodel.compile(optimizer = \n              tf.keras.optimizers.SGD(learning_rate=0.01),loss = myloss)\nhistory = model.fit(tf.zeros((100,2)),\n                    tf.ones(100),batch_size = 1,epochs = 10)  #迭代1000次\n"},{"cell_type":"code","execution_count":5,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B0A382F8D62A43A4806C98CE413DBB4F","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"x= 0.999998569\nloss= 0\n","name":"stdout"}],"source":"tf.print(\"x=\",model.x)\ntf.print(\"loss=\",model(tf.constant(0.0)))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A8C304257F0B43FB89F77CA0FA04E346","mdEditEnable":false},"source":"## 二，内置优化器"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"25F143CA91B345299643F9FA6903F8D4","mdEditEnable":false},"source":"深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\n\n在keras.optimizers子模块中，它们基本上都有对应的类的实现。\n\n* SGD, 默认参数为纯SGD, 设置momentum参数不为0实际上变成SGDM, 考虑了一阶动量, 设置 nesterov为True后变成NAG，即 Nesterov Acceleration Gradient，在计算梯度时计算的是向前走一步所在位置的梯度。\n\n* Adagrad, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率。缺点是学习率单调下降，可能后期学习速率过慢乃至提前停止学习。\n\n* RMSprop, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率，对Adagrad进行了优化，通过指数平滑只考虑一定窗口内的二阶动量。\n\n* Adadelta, 考虑了二阶动量，与RMSprop类似，但是更加复杂一些，自适应性更强。\n\n* Adam, 同时考虑了一阶动量和二阶动量，可以看成RMSprop上进一步考虑了Momentum。\n\n* Nadam, 在Adam基础上进一步考虑了 Nesterov Acceleration。"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}
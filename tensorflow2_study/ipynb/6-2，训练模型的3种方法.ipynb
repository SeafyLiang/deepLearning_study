{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4A9811E6AD8943A987C8304BFB7A7F0C","mdEditEnable":false},"source":"# 6-2,训练模型的3种方法\n\n模型的训练主要有内置fit方法、内置tran_on_batch方法、自定义训练循环。\n\n注：fit_generator方法在tf.keras中不推荐使用，其功能已经被fit包含。\n"},{"cell_type":"code","execution_count":1,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2163175B943A457498CA551C9DA7832A","collapsed":false,"scrolled":false},"outputs":[],"source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras import * \n\n#打印时间分割线\n@tf.function\ndef printbar():\n    ts = tf.timestamp()\n    today_ts = ts%(24*60*60)\n\n    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n    minite = tf.cast((today_ts%3600)//60,tf.int32)\n    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n    \n    def timeformat(m):\n        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n            return(tf.strings.format(\"0{}\",m))\n        else:\n            return(tf.strings.format(\"{}\",m))\n    \n    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n                timeformat(second)],separator = \":\")\n    tf.print(\"==========\"*8,end = \"\")\n    tf.print(timestring)\n    "},{"cell_type":"code","execution_count":2,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9DA1B953032947A9A0F323268E7C4E62","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n2113536/2110848 [==============================] - 3s 1us/step\n","name":"stdout"}],"source":"MAX_LEN = 300\nBATCH_SIZE = 32\n(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\nx_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\nx_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n\nMAX_WORDS = x_train.max()+1\nCAT_NUM = y_train.max()+1\n\nds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n   \nds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5CDF6AAEC8D245F49F72970AC119CC55","mdEditEnable":false},"source":"## 一，内置fit方法"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0C2117BC9CA541F28749AAC8BC815AE0","mdEditEnable":false},"source":"该方法功能非常强大, 支持对numpy array, tf.data.Dataset以及 Python generator数据进行训练。\n\n并且可以通过设置回调函数实现对训练过程的复杂控制逻辑。"},{"cell_type":"code","execution_count":4,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CE9822FA1119449F97ECD034F0C2E3B5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 300, 7)            216874    \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 296, 64)           2304      \n","_________________________________________________________________\n","max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 2336)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 46)                107502    \n","=================================================================\n","Total params: 332,856\n","Trainable params: 332,856\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":"tf.keras.backend.clear_session()\ndef create_model():\n    \n    model = models.Sequential()\n    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n    model.add(layers.MaxPool1D(2))\n    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n    model.add(layers.MaxPool1D(2))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n    return(model)\n\ndef compile_model(model):\n    model.compile(optimizer=optimizers.Nadam(),\n                loss=losses.SparseCategoricalCrossentropy(),\n                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n    return(model)\n \nmodel = create_model()\nmodel.summary()\nmodel = compile_model(model)\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"60902FAC96DE45D18BA902223BAA64A2","mdEditEnable":false},"source":"```\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 300, 7)            216874    \n_________________________________________________________________\nconv1d (Conv1D)              (None, 296, 64)           2304      \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 146, 32)           6176      \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2336)              0         \n_________________________________________________________________\ndense (Dense)                (None, 46)                107502    \n=================================================================\nTotal params: 332,856\nTrainable params: 332,856\nNon-trainable params: 0\n_________________________________________________________________\n```"},{"cell_type":"code","execution_count":5,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BEE07C0F9B4C4AA0A5F91B9824F53537"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train for 281 steps, validate for 71 steps\n","Epoch 1/10\n","281/281 [==============================] - 11s 39ms/step - loss: 2.0054 - sparse_categorical_accuracy: 0.4683 - sparse_top_k_categorical_accuracy: 0.7443 - val_loss: 1.6604 - val_sparse_categorical_accuracy: 0.5744 - val_sparse_top_k_categorical_accuracy: 0.7569\n","Epoch 2/10\n","281/281 [==============================] - 8s 30ms/step - loss: 1.4802 - sparse_categorical_accuracy: 0.6167 - sparse_top_k_categorical_accuracy: 0.7947 - val_loss: 1.5262 - val_sparse_categorical_accuracy: 0.6242 - val_sparse_top_k_categorical_accuracy: 0.7907\n","Epoch 3/10\n","281/281 [==============================] - 10s 34ms/step - loss: 1.2000 - sparse_categorical_accuracy: 0.6898 - sparse_top_k_categorical_accuracy: 0.8485 - val_loss: 1.5431 - val_sparse_categorical_accuracy: 0.6367 - val_sparse_top_k_categorical_accuracy: 0.8032\n","Epoch 4/10\n","281/281 [==============================] - 8s 30ms/step - loss: 0.9274 - sparse_categorical_accuracy: 0.7626 - sparse_top_k_categorical_accuracy: 0.9049 - val_loss: 1.7144 - val_sparse_categorical_accuracy: 0.6300 - val_sparse_top_k_categorical_accuracy: 0.8010\n","Epoch 5/10\n","281/281 [==============================] - 10s 34ms/step - loss: 0.6881 - sparse_categorical_accuracy: 0.8241 - sparse_top_k_categorical_accuracy: 0.9463 - val_loss: 1.9174 - val_sparse_categorical_accuracy: 0.6247 - val_sparse_top_k_categorical_accuracy: 0.7983\n","Epoch 6/10\n","281/281 [==============================] - 9s 33ms/step - loss: 0.5167 - sparse_categorical_accuracy: 0.8753 - sparse_top_k_categorical_accuracy: 0.9687 - val_loss: 2.0889 - val_sparse_categorical_accuracy: 0.6291 - val_sparse_top_k_categorical_accuracy: 0.8001\n","Epoch 7/10\n","281/281 [==============================] - 10s 35ms/step - loss: 0.4075 - sparse_categorical_accuracy: 0.9044 - sparse_top_k_categorical_accuracy: 0.9800 - val_loss: 2.2479 - val_sparse_categorical_accuracy: 0.6278 - val_sparse_top_k_categorical_accuracy: 0.8037\n","Epoch 8/10\n","281/281 [==============================] - 10s 35ms/step - loss: 0.3367 - sparse_categorical_accuracy: 0.9194 - sparse_top_k_categorical_accuracy: 0.9869 - val_loss: 2.4076 - val_sparse_categorical_accuracy: 0.6193 - val_sparse_top_k_categorical_accuracy: 0.8010\n","Epoch 9/10\n","281/281 [==============================] - 10s 35ms/step - loss: 0.2888 - sparse_categorical_accuracy: 0.9308 - sparse_top_k_categorical_accuracy: 0.9910 - val_loss: 2.5644 - val_sparse_categorical_accuracy: 0.6180 - val_sparse_top_k_categorical_accuracy: 0.7988\n","Epoch 10/10\n","281/281 [==============================] - 8s 29ms/step - loss: 0.2543 - sparse_categorical_accuracy: 0.9361 - sparse_top_k_categorical_accuracy: 0.9935 - val_loss: 2.7273 - val_sparse_categorical_accuracy: 0.6171 - val_sparse_top_k_categorical_accuracy: 0.7970\n"]}],"source":"history = model.fit(ds_train,validation_data = ds_test,epochs = 10)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3A6AF8D14CD245128CFACE7DF426EA68","mdEditEnable":false},"source":"## 二，内置train_on_batch方法"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CDBE9250C3A041E48BAC50A86CC368EF","mdEditEnable":false},"source":"该内置方法相比较fit方法更加灵活，可以不通过回调函数而直接在批次层次上更加精细地控制训练的过程。"},{"cell_type":"code","execution_count":3,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"650090BFC92443D185DE6A08FAB56266","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 300, 7)            216874    \n_________________________________________________________________\nconv1d (Conv1D)              (None, 296, 64)           2304      \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 146, 32)           6176      \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2336)              0         \n_________________________________________________________________\ndense (Dense)                (None, 46)                107502    \n=================================================================\nTotal params: 332,856\nTrainable params: 332,856\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}],"source":"tf.keras.backend.clear_session()\n\ndef create_model():\n    model = models.Sequential()\n\n    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n    model.add(layers.MaxPool1D(2))\n    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n    model.add(layers.MaxPool1D(2))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n    return(model)\n\ndef compile_model(model):\n    model.compile(optimizer=optimizers.Nadam(),\n                loss=losses.SparseCategoricalCrossentropy(),\n                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) \n    return(model)\n \nmodel = create_model()\nmodel.summary()\nmodel = compile_model(model)\n"},{"cell_type":"code","execution_count":4,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"07966068CAAA46069AC66ECF977280C9","collapsed":false,"scrolled":false},"outputs":[],"source":"def train_model(model,ds_train,ds_valid,epoches):\n\n    for epoch in tf.range(1,epoches+1):\n        model.reset_metrics()\n        \n        # 在后期降低学习率\n        if epoch == 5:\n            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n        \n        for x, y in ds_train:\n            train_result = model.train_on_batch(x, y)\n\n        for x, y in ds_valid:\n            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n            \n        if epoch%1 ==0:\n            printbar()\n            tf.print(\"epoch = \",epoch)\n            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n            print(\"\")"},{"cell_type":"code","execution_count":5,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EF7649908AFD4F938D4D348B5EB64172","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================16:44:50\nepoch =  1\ntrain: {'loss': 1.7782589, 'sparse_categorical_accuracy': 0.54545456, 'sparse_top_k_categorical_accuracy': 0.6818182}\nvalid: {'loss': 2.1192057, 'sparse_categorical_accuracy': 0.5507569, 'sparse_top_k_categorical_accuracy': 0.75779164}\n\n================================================================================16:44:57\nepoch =  2\ntrain: {'loss': 1.4705807, 'sparse_categorical_accuracy': 0.59090906, 'sparse_top_k_categorical_accuracy': 0.72727275}\nvalid: {'loss': 1.6258131, 'sparse_categorical_accuracy': 0.6046305, 'sparse_top_k_categorical_accuracy': 0.7871772}\n\n================================================================================16:45:04\nepoch =  3\ntrain: {'loss': 1.088266, 'sparse_categorical_accuracy': 0.72727275, 'sparse_top_k_categorical_accuracy': 0.8181818}\nvalid: {'loss': 1.3976628, 'sparse_categorical_accuracy': 0.6451469, 'sparse_top_k_categorical_accuracy': 0.8134461}\n\n================================================================================16:45:12\nepoch =  4\ntrain: {'loss': 0.71706825, 'sparse_categorical_accuracy': 0.77272725, 'sparse_top_k_categorical_accuracy': 0.95454544}\nvalid: {'loss': 1.4577352, 'sparse_categorical_accuracy': 0.6460374, 'sparse_top_k_categorical_accuracy': 0.81745327}\n\nLowering optimizer Learning Rate...\n\n\n================================================================================16:45:19\nepoch =  5\ntrain: {'loss': 0.45584556, 'sparse_categorical_accuracy': 0.8636364, 'sparse_top_k_categorical_accuracy': 0.95454544}\nvalid: {'loss': 1.5735245, 'sparse_categorical_accuracy': 0.65138024, 'sparse_top_k_categorical_accuracy': 0.8165628}\n\n================================================================================16:45:26\nepoch =  6\ntrain: {'loss': 0.360793, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\nvalid: {'loss': 1.673664, 'sparse_categorical_accuracy': 0.650935, 'sparse_top_k_categorical_accuracy': 0.81478184}\n\n================================================================================16:45:33\nepoch =  7\ntrain: {'loss': 0.29006183, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\nvalid: {'loss': 1.7496933, 'sparse_categorical_accuracy': 0.6438112, 'sparse_top_k_categorical_accuracy': 0.817008}\n\n================================================================================16:45:40\nepoch =  8\ntrain: {'loss': 0.23076382, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}\nvalid: {'loss': 1.8100835, 'sparse_categorical_accuracy': 0.64069456, 'sparse_top_k_categorical_accuracy': 0.81745327}\n\n================================================================================16:45:48\nepoch =  9\ntrain: {'loss': 0.18231665, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\nvalid: {'loss': 1.8627172, 'sparse_categorical_accuracy': 0.6398041, 'sparse_top_k_categorical_accuracy': 0.8161175}\n\n================================================================================16:45:55\nepoch =  10\ntrain: {'loss': 0.14538175, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\nvalid: {'loss': 1.9056457, 'sparse_categorical_accuracy': 0.6375779, 'sparse_top_k_categorical_accuracy': 0.8156723}\n\n","name":"stdout"}],"source":"train_model(model,ds_train,ds_test,10)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"97D7BCEB6B5047DD8729D8F83ED4C11E","mdEditEnable":false},"source":"## 三，自定义训练循环"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E53C624074B2412BBBED219B503F00EB","mdEditEnable":false},"source":"自定义训练循环无需编译模型，直接利用优化器根据损失函数反向传播迭代参数，拥有最高的灵活性。"},{"cell_type":"code","execution_count":6,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B6F852055F7F4AEF8965E3801B2B8287","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 300, 7)            216874    \n_________________________________________________________________\nconv1d (Conv1D)              (None, 296, 64)           2304      \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 146, 32)           6176      \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2336)              0         \n_________________________________________________________________\ndense (Dense)                (None, 46)                107502    \n=================================================================\nTotal params: 332,856\nTrainable params: 332,856\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}],"source":"tf.keras.backend.clear_session()\n\ndef create_model():\n    \n    model = models.Sequential()\n\n    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n    model.add(layers.MaxPool1D(2))\n    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n    model.add(layers.MaxPool1D(2))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n    return(model)\n\nmodel = create_model()\nmodel.summary()"},{"cell_type":"code","execution_count":7,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"38A8AF8E01934C088439701E275DBD4D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================16:46:18\nEpoch=1,Loss:2.00627327,Accuracy:0.46504119,Valid Loss:1.69869936,Valid Accuracy:0.553873539\n\n================================================================================16:46:25\nEpoch=2,Loss:1.46558726,Accuracy:0.621131122,Valid Loss:1.55003631,Valid Accuracy:0.609973311\n\n================================================================================16:46:32\nEpoch=3,Loss:1.18542743,Accuracy:0.688376725,Valid Loss:1.56454027,Valid Accuracy:0.646482646\n\n================================================================================16:46:39\nEpoch=4,Loss:0.912428439,Accuracy:0.761523068,Valid Loss:1.75646842,Valid Accuracy:0.646927893\n\n================================================================================16:46:46\nEpoch=5,Loss:0.672883391,Accuracy:0.82654196,Valid Loss:2.02260804,Valid Accuracy:0.636687458\n\n================================================================================16:46:53\nEpoch=6,Loss:0.513099134,Accuracy:0.875417531,Valid Loss:2.30667543,Valid Accuracy:0.626447\n\n================================================================================16:47:00\nEpoch=7,Loss:0.412734926,Accuracy:0.90236026,Valid Loss:2.55101776,Valid Accuracy:0.624220848\n\n================================================================================16:47:07\nEpoch=8,Loss:0.344970435,Accuracy:0.917279,Valid Loss:2.73618364,Valid Accuracy:0.621549428\n\n================================================================================16:47:14\nEpoch=9,Loss:0.299876332,Accuracy:0.927521706,Valid Loss:2.86248517,Valid Accuracy:0.621549428\n\n================================================================================16:47:21\nEpoch=10,Loss:0.267550141,Accuracy:0.934313059,Valid Loss:2.94415259,Valid Accuracy:0.623330355\n\n","name":"stdout"}],"source":"optimizer = optimizers.Nadam()\nloss_func = losses.SparseCategoricalCrossentropy()\n\ntrain_loss = metrics.Mean(name='train_loss')\ntrain_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\nvalid_loss = metrics.Mean(name='valid_loss')\nvalid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n\n@tf.function\ndef train_step(model, features, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(features,training = True)\n        loss = loss_func(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    train_loss.update_state(loss)\n    train_metric.update_state(labels, predictions)\n    \n\n@tf.function\ndef valid_step(model, features, labels):\n    predictions = model(features)\n    batch_loss = loss_func(labels, predictions)\n    valid_loss.update_state(batch_loss)\n    valid_metric.update_state(labels, predictions)\n    \n\ndef train_model(model,ds_train,ds_valid,epochs):\n    for epoch in tf.range(1,epochs+1):\n        \n        for features, labels in ds_train:\n            train_step(model,features,labels)\n\n        for features, labels in ds_valid:\n            valid_step(model,features,labels)\n\n        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n        \n        if epoch%1 ==0:\n            printbar()\n            tf.print(tf.strings.format(logs,\n            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n            tf.print(\"\")\n            \n        train_loss.reset_states()\n        valid_loss.reset_states()\n        train_metric.reset_states()\n        valid_metric.reset_states()\n\ntrain_model(model,ds_train,ds_test,10)\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}
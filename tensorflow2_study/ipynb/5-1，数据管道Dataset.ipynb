{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9B48ED03315344718788ABAC354E5FFE","mdEditEnable":false},"source":"# 5-1,数据管道Dataset\n\n如果需要训练的数据大小不大，例如不到1G，那么可以直接全部读入内存中进行训练，这样一般效率最高。\n\n但如果需要训练的数据很大，例如超过10G，无法一次载入内存，那么通常需要在训练的过程中分批逐渐读入。\n\n使用 tf.data API 可以构建数据输入管道，轻松处理大量的数据，不同的数据格式，以及不同的数据转换。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2C0289A27324430CA3B644790D7AA260","mdEditEnable":false},"source":"## 一，构建数据管道"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"41A64E930A0F4CD585CEE5D492E0935B","mdEditEnable":false},"source":"可以从 Numpy array, Pandas DataFrame, Python generator, csv文件, 文本文件, 文件路径, tfrecords文件等方式构建数据管道。\n\n其中通过Numpy array, Pandas DataFrame, 文件路径构建数据管道是最常用的方法。\n\n通过tfrecords文件方式构建数据管道较为复杂，需要对样本构建tf.Example后压缩成字符串写到tfrecoreds文件，读取后再解析成tf.Example。\n\n但tfrecoreds文件的优点是压缩后文件较小，便于网络传播，加载速度较快。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2526281BF5214E4FBA5059A1DE7C2F49","mdEditEnable":false},"source":"### 1,从Numpy array构建数据管道"},{"cell_type":"code","execution_count":1,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DAACBE0EAF734F52ADF9993D3A1929BE","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n","name":"stdout"}],"source":"# 从Numpy array构建数据管道\n\nimport tensorflow as tf\nimport numpy as np \nfrom sklearn import datasets \niris = datasets.load_iris()\n\n\nds1 = tf.data.Dataset.from_tensor_slices((iris[\"data\"],iris[\"target\"]))\nfor features,label in ds1.take(5):\n    print(features,label)\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"64EB52AC225E4407805FF172E8FB6431","mdEditEnable":false},"source":"### 2,从 Pandas DataFrame构建数据管道"},{"cell_type":"code","execution_count":2,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A3CE680DA0A54AC08535A5E8896C29B4","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=5.1>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.5>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.9>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.7>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.2>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.3>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n","name":"stdout"}],"source":"# 从 Pandas DataFrame构建数据管道\nimport tensorflow as tf\nfrom sklearn import datasets \nimport pandas as pd\niris = datasets.load_iris()\ndfiris = pd.DataFrame(iris[\"data\"],columns = iris.feature_names)\nds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict(\"list\"),iris[\"target\"]))\n\nfor features,label in ds2.take(3):\n    print(features,label)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6D97198B0F4C405587A1CA05495C86BC","mdEditEnable":false},"source":"### 3,从Python generator构建数据管道"},{"cell_type":"code","execution_count":3,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"61DACFCA2F2743E984FCFFD88A07F681","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Found 2000 images belonging to 2 classes.\n{'airplane': 0, 'automobile': 1}\n","name":"stdout"}],"source":"# 从Python generator构建数据管道\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# 定义一个从文件中读取图片的generator\nimage_generator = ImageDataGenerator(rescale=1.0/255).flow_from_directory(\n                    \"/home/kesci/input/data3483/data/cifar2/test/\",\n                    target_size=(32, 32),\n                    batch_size=20,\n                    class_mode='binary')\n\nclassdict = image_generator.class_indices\nprint(classdict)\n\ndef generator():\n    for features,label in image_generator:\n        yield (features,label)\n\nds3 = tf.data.Dataset.from_generator(generator,output_types=(tf.float32,tf.int32))"},{"cell_type":"code","execution_count":4,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"12F12A29DB244A64BB482F24A0272338","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 432x432 with 9 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/12F12A29DB244A64BB482F24A0272338/q9fy2b5ki.svg\">"},"transient":{}}],"source":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nplt.figure(figsize=(6,6)) \nfor i,(img,label) in enumerate(ds3.unbatch().take(9)):\n    ax=plt.subplot(3,3,i+1)\n    ax.imshow(img.numpy())\n    ax.set_title(\"label = %d\"%label)\n    ax.set_xticks([])\n    ax.set_yticks([]) \nplt.show()"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8C08D45B43CA438497FFFCE47AC35E63","mdEditEnable":false},"source":"### 4,从csv文件构建数据管道"},{"cell_type":"code","execution_count":5,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9769FDF39A5D450C8F8DA972CA8412C0","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"OrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([136,  48, 805], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 3, 3], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'Richard, Mr. Emile', b\"O'Driscoll, Miss. Bridget\",\n       b'Hedman, Mr. Oskar Arvid'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'male', b'female', b'male'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([23.,  0., 27.], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'SC/PARIS 2133', b'14311', b'347089'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([15.0458,  7.75  ,  6.975 ], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'', b'', b''], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'C', b'Q', b'S'], dtype=object)>)]) tf.Tensor([0 1 1], shape=(3,), dtype=int32)\nOrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([875, 188, 139], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 1, 3], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'Abelson, Mrs. Samuel (Hannah Wizosky)',\n       b'Romaine, Mr. Charles Hallace (\"Mr C Rolmane\")',\n       b'Osen, Mr. Olaf Elon'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'female', b'male', b'male'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([28., 45., 16.], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 0, 0], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'P/PP 3381', b'111428', b'7534'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([24.    , 26.55  ,  9.2167], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'', b'', b''], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'C', b'S', b'S'], dtype=object)>)]) tf.Tensor([1 1 0], shape=(3,), dtype=int32)\n","name":"stdout"}],"source":"# 从csv文件构建数据管道\nds4 = tf.data.experimental.make_csv_dataset(\n      file_pattern = [\"/home/kesci/input/data3483/data/titanic/train.csv\",\"/home/kesci/input/data3483/data/titanic/test.csv\"],\n      batch_size=3, \n      label_name=\"Survived\",\n      na_value=\"\",\n      num_epochs=1,\n      ignore_errors=True)\n\nfor data,label in ds4.take(2):\n    print(data,label)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5D1A5B0507FF415981C690B3BDDAEF0E","mdEditEnable":false},"source":"### 5,从文本文件构建数据管道"},{"cell_type":"code","execution_count":6,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"62F86DB2F126416183D674BD9092F3CA","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\ntf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\ntf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\ntf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\ntf.Tensor(b'687,0,3,\"Panula, Mr. Jaako Arnold\",male,14.0,4,1,3101295,39.6875,,S', shape=(), dtype=string)\n","name":"stdout"}],"source":"# 从文本文件构建数据管道\n\nds5 = tf.data.TextLineDataset(\n    filenames = [\"/home/kesci/input/data3483/data/titanic/train.csv\",\"/home/kesci/input/data3483/data/titanic/test.csv\"]\n    ).skip(1) #略去第一行header\n\nfor line in ds5.take(5):\n    print(line)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AA5E14F2E2A343A5B06DBCEFF3E778AB","mdEditEnable":false},"source":"### 6,从文件路径构建数据管道"},{"cell_type":"code","execution_count":7,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"485438FCFD464592883D0173E2962F62","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(b'/home/kesci/input/data3483/data/cifar2/train/airplane/4266.jpg', shape=(), dtype=string)\ntf.Tensor(b'/home/kesci/input/data3483/data/cifar2/train/airplane/4131.jpg', shape=(), dtype=string)\ntf.Tensor(b'/home/kesci/input/data3483/data/cifar2/train/automobile/764.jpg', shape=(), dtype=string)\ntf.Tensor(b'/home/kesci/input/data3483/data/cifar2/train/automobile/1303.jpg', shape=(), dtype=string)\ntf.Tensor(b'/home/kesci/input/data3483/data/cifar2/train/airplane/913.jpg', shape=(), dtype=string)\n","name":"stdout"}],"source":"ds6 = tf.data.Dataset.list_files(\"/home/kesci/input/data3483/data/cifar2/train/*/*.jpg\")\nfor file in ds6.take(5):\n    print(file)"},{"cell_type":"code","execution_count":8,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"31453C87A375413D9670AB95D69DDBF1","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/31453C87A375413D9670AB95D69DDBF1/q9fy8a84w5.svg\">"},"transient":{}},{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/31453C87A375413D9670AB95D69DDBF1/q9fy8ab9qb.svg\">"},"transient":{}}],"source":"from matplotlib import pyplot as plt \ndef load_image(img_path,size = (32,32)):\n    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img) #注意此处为jpeg格式\n    img = tf.image.resize(img,size)\n    return(img,label)\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nfor i,(img,label) in enumerate(ds6.map(load_image).take(2)):\n    plt.figure(i)\n    plt.imshow((img/255.0).numpy())\n    plt.title(\"label = %d\"%label)\n    plt.xticks([])\n    plt.yticks([])"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5F0CD2B81FD942EDAB1848B34A615B18","mdEditEnable":false},"source":"### 7,从tfrecords文件构建数据管道"},{"metadata":{"id":"AEAE9E1C13CD48EFAEBA62BA05F39E23","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import os\nimport numpy as np\n\n# inpath：原始数据路径 outpath:TFRecord文件输出路径\ndef create_tfrecords(inpath,outpath): \n    writer = tf.io.TFRecordWriter(outpath)\n    dirs = os.listdir(inpath)\n    for index, name in enumerate(dirs):\n        class_path = inpath +\"/\"+ name+\"/\"\n        for img_name in os.listdir(class_path):\n            img_path = class_path + img_name\n            img = tf.io.read_file(img_path)\n            #img = tf.image.decode_image(img)\n            #img = tf.image.encode_jpeg(img) #统一成jpeg格式压缩\n            example = tf.train.Example(\n               features=tf.train.Features(feature={\n                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n                    'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))\n               }))\n            writer.write(example.SerializeToString())\n    writer.close()\n    \ncreate_tfrecords(\"/home/kesci/input/data3483/data/cifar2/test\",\"/home/kesci/input/data3483/datacifar2_test.tfrecords/\")","execution_count":null},{"cell_type":"code","execution_count":16,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4D82F08852BC4D618CF930E242494E60","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 432x432 with 9 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/4D82F08852BC4D618CF930E242494E60/q9fyfeggde.svg\">"},"transient":{}}],"source":"from matplotlib import pyplot as plt \n\ndef parse_example(proto):\n    description ={ 'img_raw' : tf.io.FixedLenFeature([], tf.string),\n                   'label': tf.io.FixedLenFeature([], tf.int64)} \n    example = tf.io.parse_single_example(proto, description)\n    img = tf.image.decode_jpeg(example[\"img_raw\"])   #注意此处为jpeg格式\n    img = tf.image.resize(img, (32,32))\n    label = example[\"label\"]\n    return(img,label)\n\nds7 = tf.data.TFRecordDataset(\"/home/kesci/input/data3483/data/cifar2_test.tfrecords\").map(parse_example).shuffle(3000)\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nplt.figure(figsize=(6,6)) \nfor i,(img,label) in enumerate(ds7.take(9)):\n    ax=plt.subplot(3,3,i+1)\n    ax.imshow((img/255.0).numpy())\n    ax.set_title(\"label = %d\"%label)\n    ax.set_xticks([])\n    ax.set_yticks([]) \nplt.show()\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"908354945A52475C92B6ADBAD6CA752A","mdEditEnable":false},"source":"## 二，应用数据转换"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"392F05AB10384C4B822FA4D56B90779A","mdEditEnable":false},"source":"Dataset数据结构应用非常灵活，因为它本质上是一个Sequece序列，其每个元素可以是各种类型，例如可以是张量，列表，字典，也可以是Dataset。\n\nDataset包含了非常丰富的数据转换功能。\n\n* map: 将转换函数映射到数据集每一个元素。\n\n* flat_map: 将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。\n\n* interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。\n\n* filter: 过滤掉某些元素。\n\n* zip: 将两个长度相同的Dataset横向铰合。\n\n* concatenate: 将两个Dataset纵向连接。\n\n* reduce: 执行归并操作。\n\n* batch : 构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。\n\n* padded_batch: 构建批次，类似batch, 但可以填充到相同的形状。\n\n* window :构建滑动窗口，返回Dataset of Dataset.\n\n* shuffle: 数据顺序洗牌。\n\n* repeat: 重复数据若干次，不带参数时，重复无数次。\n\n* shard: 采样，从某个位置开始隔固定距离采样一个元素。\n\n* take: 采样，从开始位置取前几个元素。\n"},{"cell_type":"code","execution_count":17,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8BE924A708944C789813D8AEB2FF7ABE","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\ntf.Tensor([b'hello' b'China'], shape=(2,), dtype=string)\ntf.Tensor([b'hello' b'Beijing'], shape=(2,), dtype=string)\n","name":"stdout"}],"source":"#map:将转换函数映射到数据集每一个元素\n\nds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\nds_map = ds.map(lambda x:tf.strings.split(x,\" \"))\nfor x in ds_map:\n    print(x)"},{"cell_type":"code","execution_count":18,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"79DEF8A470DB4C5284F6D3CC158F6167","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(b'hello', shape=(), dtype=string)\ntf.Tensor(b'world', shape=(), dtype=string)\ntf.Tensor(b'hello', shape=(), dtype=string)\ntf.Tensor(b'China', shape=(), dtype=string)\ntf.Tensor(b'hello', shape=(), dtype=string)\ntf.Tensor(b'Beijing', shape=(), dtype=string)\n","name":"stdout"}],"source":"#flat_map:将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。\n\nds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\nds_flatmap = ds.flat_map(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,\" \")))\nfor x in ds_flatmap:\n    print(x)"},{"cell_type":"code","execution_count":19,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4A9FB8D536AC45538E25F0DA3868BCE7","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(b'hello', shape=(), dtype=string)\ntf.Tensor(b'hello', shape=(), dtype=string)\ntf.Tensor(b'hello', shape=(), dtype=string)\ntf.Tensor(b'world', shape=(), dtype=string)\ntf.Tensor(b'China', shape=(), dtype=string)\ntf.Tensor(b'Beijing', shape=(), dtype=string)\n","name":"stdout"}],"source":"# interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。\n\nds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\nds_interleave = ds.interleave(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,\" \")))\nfor x in ds_interleave:\n    print(x)\n    "},{"cell_type":"code","execution_count":20,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C2FB16EB76DB497BA139D8E89351B8C6","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(b'hello China', shape=(), dtype=string)\ntf.Tensor(b'hello Beijing', shape=(), dtype=string)\n","name":"stdout"}],"source":"#filter:过滤掉某些元素。\n\nds = tf.data.Dataset.from_tensor_slices([\"hello world\",\"hello China\",\"hello Beijing\"])\n#找出含有字母a或B的元素\nds_filter = ds.filter(lambda x: tf.strings.regex_full_match(x, \".*[a|B].*\"))\nfor x in ds_filter:\n    print(x)\n    "},{"cell_type":"code","execution_count":21,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AD959D1C6D3144899F2D1A41C0D3FD64","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"0 3 6\n1 4 7\n2 5 8\n","name":"stdout"}],"source":"#zip:将两个长度相同的Dataset横向铰合。\n\nds1 = tf.data.Dataset.range(0,3)\nds2 = tf.data.Dataset.range(3,6)\nds3 = tf.data.Dataset.range(6,9)\nds_zip = tf.data.Dataset.zip((ds1,ds2,ds3))\nfor x,y,z in ds_zip:\n    print(x.numpy(),y.numpy(),z.numpy())\n"},{"cell_type":"code","execution_count":22,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F9FD2E31EE6A4F4FB95011C0D6EC2763","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(3, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\ntf.Tensor(5, shape=(), dtype=int64)\n","name":"stdout"}],"source":"#condatenate:将两个Dataset纵向连接。\n\nds1 = tf.data.Dataset.range(0,3)\nds2 = tf.data.Dataset.range(3,6)\nds_concat = tf.data.Dataset.concatenate(ds1,ds2)\nfor x in ds_concat:\n    print(x)"},{"cell_type":"code","execution_count":23,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"095983E15D0442CA90DA94D775DCE742","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"<tf.Tensor: shape=(), dtype=float32, numpy=15.0>"},"transient":{}}],"source":"#reduce:执行归并操作。\n\nds = tf.data.Dataset.from_tensor_slices([1,2,3,4,5.0])\nresult = ds.reduce(0.0,lambda x,y:tf.add(x,y))\nresult"},{"cell_type":"code","execution_count":24,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4707C8C9B2F44FF0B87D44C9A618FFC1","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)\ntf.Tensor([4 5 6 7], shape=(4,), dtype=int64)\ntf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)\n","name":"stdout"}],"source":"#batch:构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。 \n\nds = tf.data.Dataset.range(12)\nds_batch = ds.batch(4)\nfor x in ds_batch:\n    print(x)"},{"cell_type":"code","execution_count":25,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B028417F41B54BDF8BE49B182E971842","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(\n[[1 2 0 0]\n [3 4 5 0]], shape=(2, 4), dtype=int32)\ntf.Tensor(\n[[6 7 0 0]\n [8 0 0 0]], shape=(2, 4), dtype=int32)\n","name":"stdout"}],"source":"#padded_batch:构建批次，类似batch, 但可以填充到相同的形状。\n\nelements = [[1, 2],[3, 4, 5],[6, 7],[8]]\nds = tf.data.Dataset.from_generator(lambda: iter(elements), tf.int32)\n\nds_padded_batch = ds.padded_batch(2,padded_shapes = [4,])\nfor x in ds_padded_batch:\n    print(x)    "},{"cell_type":"code","execution_count":26,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"56B77DB7E82D427B9C7CCE281EB60E36","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor([0 1 2], shape=(3,), dtype=int64)\ntf.Tensor([1 2 3], shape=(3,), dtype=int64)\ntf.Tensor([2 3 4], shape=(3,), dtype=int64)\ntf.Tensor([3 4 5], shape=(3,), dtype=int64)\ntf.Tensor([4 5 6], shape=(3,), dtype=int64)\ntf.Tensor([5 6 7], shape=(3,), dtype=int64)\ntf.Tensor([6 7 8], shape=(3,), dtype=int64)\ntf.Tensor([7 8 9], shape=(3,), dtype=int64)\ntf.Tensor([ 8  9 10], shape=(3,), dtype=int64)\ntf.Tensor([ 9 10 11], shape=(3,), dtype=int64)\n","name":"stdout"}],"source":"#window:构建滑动窗口，返回Dataset of Dataset.\n\nds = tf.data.Dataset.range(12)\n#window返回的是Dataset of Dataset,可以用flat_map压平\nds_window = ds.window(3, shift=1).flat_map(lambda x: x.batch(3,drop_remainder=True)) \nfor x in ds_window:\n    print(x)"},{"cell_type":"code","execution_count":27,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"731A2221BB5441AA877E0AFC7AAAD3A7","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\ntf.Tensor(3, shape=(), dtype=int64)\ntf.Tensor(7, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(6, shape=(), dtype=int64)\ntf.Tensor(10, shape=(), dtype=int64)\ntf.Tensor(8, shape=(), dtype=int64)\ntf.Tensor(5, shape=(), dtype=int64)\ntf.Tensor(11, shape=(), dtype=int64)\ntf.Tensor(9, shape=(), dtype=int64)\n","name":"stdout"}],"source":"#shuffle:数据顺序洗牌。\n\nds = tf.data.Dataset.range(12)\nds_shuffle = ds.shuffle(buffer_size = 5)\nfor x in ds_shuffle:\n    print(x)\n    "},{"cell_type":"code","execution_count":28,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BD7F1E2D2FE54D698052C0FC88E53241","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\n","name":"stdout"}],"source":"#repeat:重复数据若干次，不带参数时，重复无数次。\n\nds = tf.data.Dataset.range(3)\nds_repeat = ds.repeat(3)\nfor x in ds_repeat:\n    print(x)"},{"cell_type":"code","execution_count":29,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DF56FB8A2AF947B083F2E5E29EDB0052","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\ntf.Tensor(7, shape=(), dtype=int64)\ntf.Tensor(10, shape=(), dtype=int64)\n","name":"stdout"}],"source":"#shard:采样，从某个位置开始隔固定距离采样一个元素。\n\nds = tf.data.Dataset.range(12)\nds_shard = ds.shard(3,index = 1)\n\nfor x in ds_shard:\n    print(x)"},{"cell_type":"code","execution_count":30,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1C7A817C05254BF5A23F933C9D557EF6","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[0, 1, 2]"},"transient":{}}],"source":"#take:采样，从开始位置取前几个元素。\n\nds = tf.data.Dataset.range(12)\nds_take = ds.take(3)\n\nlist(ds_take.as_numpy_iterator())\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E3DDE4A70029464DAA64E4553842EB77","mdEditEnable":false},"source":"## 三，提升管道性能"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"928269B33F474F518D150BFCDB4BFF2C","mdEditEnable":false},"source":"训练深度学习模型常常会非常耗时。\n\n模型训练的耗时主要来自于两个部分，一部分来自**数据准备**，另一部分来自**参数迭代**。\n\n参数迭代过程的耗时通常依赖于GPU来提升。\n\n而数据准备过程的耗时则可以通过构建高效的数据管道进行提升。\n\n以下是一些构建高效数据管道的建议。\n\n* 1，使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。\n\n* 2，使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。\n\n* 3，使用 map 时设置num_parallel_calls 让数据转换过程多进行执行。\n\n* 4，使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。\n\n* 5，使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3585905568DD4F7DA716BC164CBE3A99","mdEditEnable":false},"source":"### 1，使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。"},{"cell_type":"code","execution_count":31,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9DE53B2EAA4B4FF88B6A33F061258EEF","collapsed":false,"scrolled":false},"outputs":[],"source":"import tensorflow as tf\n\n#打印时间分割线\n@tf.function\ndef printbar():\n    ts = tf.timestamp()\n    today_ts = ts%(24*60*60)\n\n    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n    minite = tf.cast((today_ts%3600)//60,tf.int32)\n    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n    \n    def timeformat(m):\n        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n            return(tf.strings.format(\"0{}\",m))\n        else:\n            return(tf.strings.format(\"{}\",m))\n    \n    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n                timeformat(second)],separator = \":\")\n    tf.print(\"==========\"*8,end = \"\")\n    tf.print(timestring)\n    "},{"cell_type":"code","execution_count":32,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B943A7146F614E70BDCDE75AAAB53CCD","collapsed":false,"scrolled":false},"outputs":[],"source":"import time\n\n# 数据准备和参数迭代两个过程默认情况下是串行的。\n\n# 模拟数据准备\ndef generator():\n    for i in range(10):\n        #假设每次准备数据需要2s\n        time.sleep(2) \n        yield i \nds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))\n\n# 模拟参数迭代\ndef train_step():\n    #假设每一步训练需要1s\n    time.sleep(1) \n    "},{"cell_type":"code","execution_count":33,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F1EA62D2EA72419E831148570A7D481B","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================18:02:43\nstart training...\n================================================================================18:03:13\nend training...\n","name":"stdout"}],"source":"# 训练过程预计耗时 10*2+10*1+ = 30s\nprintbar()\ntf.print(tf.constant(\"start training...\"))\nfor x in ds:\n    train_step()  \nprintbar()\ntf.print(tf.constant(\"end training...\"))"},{"cell_type":"code","execution_count":34,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F77C81DA398845208BD7C1EEE381847E","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================18:03:13\nstart training with prefetch...\n================================================================================18:03:35\nend training...\n","name":"stdout"}],"source":"# 使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。\n\n# 训练过程预计耗时 max(10*2,10*1) = 20s\nprintbar()\ntf.print(tf.constant(\"start training with prefetch...\"))\n\n# tf.data.experimental.AUTOTUNE 可以让程序自动选择合适的参数\nfor x in ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE):\n    train_step()  \n    \nprintbar()\ntf.print(tf.constant(\"end training...\"))\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"88B4BECB467F496EBCE711876F3502DE","mdEditEnable":false},"source":"### 2，使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。"},{"cell_type":"code","execution_count":35,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CEB98C77C7FC4BB18A5C795268EDD101","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"tf.Tensor(b'181,0,3,\"Sage, Miss. Constance Gladys\",female,,8,2,CA. 2343,69.55,,S', shape=(), dtype=string)\ntf.Tensor(b'405,0,3,\"Oreskovic, Miss. Marija\",female,20.0,0,0,315096,8.6625,,S', shape=(), dtype=string)\ntf.Tensor(b'635,0,3,\"Skoog, Miss. Mabel\",female,9.0,3,2,347088,27.9,,S', shape=(), dtype=string)\ntf.Tensor(b'701,1,1,\"Astor, Mrs. John Jacob (Madeleine Talmadge Force)\",female,18.0,1,0,PC 17757,227.525,C62 C64,C', shape=(), dtype=string)\n","name":"stdout"}],"source":"ds_files = tf.data.Dataset.list_files(\"/home/kesci/input/data3483/data/titanic/*.csv\")\nds = ds_files.flat_map(lambda x:tf.data.TextLineDataset(x).skip(1))\nfor line in ds.take(4):\n    print(line)"},{"cell_type":"code","execution_count":36,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F5F7C0D47E5C460F8FC8C964E2BC710B","collapsed":false,"scrolled":true},"outputs":[{"output_type":"stream","text":"tf.Tensor(b'493,0,1,\"Molson, Mr. Harry Markland\",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)\ntf.Tensor(b'181,0,3,\"Sage, Miss. Constance Gladys\",female,,8,2,CA. 2343,69.55,,S', shape=(), dtype=string)\ntf.Tensor(b'53,1,1,\"Harper, Mrs. Henry Sleeper (Myna Haxtun)\",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)\ntf.Tensor(b'405,0,3,\"Oreskovic, Miss. Marija\",female,20.0,0,0,315096,8.6625,,S', shape=(), dtype=string)\ntf.Tensor(b'388,1,2,\"Buss, Miss. Kate\",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)\ntf.Tensor(b'635,0,3,\"Skoog, Miss. Mabel\",female,9.0,3,2,347088,27.9,,S', shape=(), dtype=string)\ntf.Tensor(b'192,0,2,\"Carbines, Mr. William\",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)\ntf.Tensor(b'701,1,1,\"Astor, Mrs. John Jacob (Madeleine Talmadge Force)\",female,18.0,1,0,PC 17757,227.525,C62 C64,C', shape=(), dtype=string)\n","name":"stdout"}],"source":"ds_files = tf.data.Dataset.list_files(\"/home/kesci/input/data3483/data/titanic/*.csv\")\nds = ds_files.interleave(lambda x:tf.data.TextLineDataset(x).skip(1))\nfor line in ds.take(8):\n    print(line)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FD625C036D40462F8D7B67FC32E137BB","mdEditEnable":false},"source":"### 3，使用 map 时设置num_parallel_calls 让数据转换过程多进行执行。"},{"cell_type":"code","execution_count":38,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"905514BCB3F34F42876EEB62DC30ADBA","collapsed":false,"scrolled":false},"outputs":[],"source":"ds = tf.data.Dataset.list_files(\"/home/kesci/input/data3483/data/cifar2/train/*/*.jpg\")\ndef load_image(img_path,size = (32,32)):\n    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img) #注意此处为jpeg格式\n    img = tf.image.resize(img,size)\n    return(img,label)"},{"cell_type":"code","execution_count":39,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"32AF6C37342F4B8A8D33A7C9856F36C4","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================18:05:38\nstart transformation...\n================================================================================18:05:41\nend transformation...\n","name":"stdout"}],"source":"#单进程转换\nprintbar()\ntf.print(tf.constant(\"start transformation...\"))\n\nds_map = ds.map(load_image)\nfor _ in ds_map:\n    pass\n\nprintbar()\ntf.print(tf.constant(\"end transformation...\"))"},{"cell_type":"code","execution_count":40,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"453B5FC358014983B085DD07E34A7B4B","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================18:05:46\nstart parallel transformation...\n================================================================================18:05:48\nend parallel transformation...\n","name":"stdout"}],"source":"#多进程转换\nprintbar()\ntf.print(tf.constant(\"start parallel transformation...\"))\n\nds_map_parallel = ds.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)\nfor _ in ds_map_parallel:\n    pass\n\nprintbar()\ntf.print(tf.constant(\"end parallel transformation...\"))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BE32A8B322664112855263EC560973F7","mdEditEnable":false},"source":"### 4，使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。"},{"cell_type":"code","execution_count":41,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3EDE2CF5062C490BAC9211560A6ED606","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================18:06:00\nstart training...\n================================================================================18:06:10\nepoch = 0  ended\n================================================================================18:06:20\nepoch = 1  ended\n================================================================================18:06:30\nepoch = 2  ended\n================================================================================18:06:30\nend training...\n","name":"stdout"}],"source":"import time\n\n# 模拟数据准备\ndef generator():\n    for i in range(5):\n        #假设每次准备数据需要2s\n        time.sleep(2) \n        yield i \nds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))\n\n# 模拟参数迭代\ndef train_step():\n    #假设每一步训练需要0s\n    pass\n\n# 训练过程预计耗时 (5*2+5*0)*3 = 30s\nprintbar()\ntf.print(tf.constant(\"start training...\"))\nfor epoch in tf.range(3):\n    for x in ds:\n        train_step()  \n    printbar()\n    tf.print(\"epoch =\",epoch,\" ended\")\nprintbar()\ntf.print(tf.constant(\"end training...\"))\n"},{"cell_type":"code","execution_count":42,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"072C61EB77C140B18E796B95950763E7","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================18:06:30\nstart training...\n================================================================================18:06:40\nepoch = 0  ended\n================================================================================18:06:40\nepoch = 1  ended\n================================================================================18:06:40\nepoch = 2  ended\n================================================================================18:06:40\nend training...\n","name":"stdout"}],"source":"import time\n\n# 模拟数据准备\ndef generator():\n    for i in range(5):\n        #假设每次准备数据需要2s\n        time.sleep(2) \n        yield i \n\n# 使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。\nds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32)).cache()\n\n# 模拟参数迭代\ndef train_step():\n    #假设每一步训练需要0s\n    time.sleep(0) \n\n# 训练过程预计耗时 (5*2+5*0)+(5*0+5*0)*2 = 10s\nprintbar()\ntf.print(tf.constant(\"start training...\"))\nfor epoch in tf.range(3):\n    for x in ds:\n        train_step()  \n    printbar()\n    tf.print(\"epoch =\",epoch,\" ended\")\nprintbar()\ntf.print(tf.constant(\"end training...\"))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7BE7E1FBF7F04535BBFA50422BA92E24","mdEditEnable":false},"source":"### 5，使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。"},{"cell_type":"code","execution_count":43,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"00ABEFE8EB80420A83CB73DC2FE6FC4A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================18:06:40\nstart scalar transformation...\n================================================================================18:06:46\nend scalar transformation...\n","name":"stdout"}],"source":"#先map后batch\nds = tf.data.Dataset.range(100000)\nds_map_batch = ds.map(lambda x:x**2).batch(20)\n\nprintbar()\ntf.print(tf.constant(\"start scalar transformation...\"))\nfor x in ds_map_batch:\n    pass\nprintbar()\ntf.print(tf.constant(\"end scalar transformation...\"))\n"},{"cell_type":"code","execution_count":44,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"99153CA7F956408F89CCFD034652E8CC","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================18:06:46\nstart vector transformation...\n================================================================================18:06:47\nend vector transformation...\n","name":"stdout"}],"source":"#先batch后map\nds = tf.data.Dataset.range(100000)\nds_batch_map = ds.batch(20).map(lambda x:x**2)\n\nprintbar()\ntf.print(tf.constant(\"start vector transformation...\"))\nfor x in ds_batch_map:\n    pass\nprintbar()\ntf.print(tf.constant(\"end vector transformation...\"))\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}
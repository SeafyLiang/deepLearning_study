{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"60F728737214431B91D0B848A62B5B39","mdEditEnable":false,"trusted":true},"source":"# 6-3,使用单GPU训练模型\n\n深度学习的训练过程常常非常耗时，一个模型训练几个小时是家常便饭，训练几天也是常有的事情，有时候甚至要训练几十天。\n\n训练过程的耗时主要来自于两个部分，一部分来自数据准备，另一部分来自参数迭代。\n\n当数据准备过程还是模型训练时间的主要瓶颈时，我们可以使用更多进程来准备数据。\n\n当参数迭代过程成为训练时间的主要瓶颈时，我们通常的方法是应用GPU或者Google的TPU来进行加速。\n\n详见《用GPU加速Keras模型——Colab免费GPU使用攻略》\n\nhttps://zhuanlan.zhihu.com/p/68509398"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"53DC05A2AFAA459D80738ADA5E4BEF69","trusted":true},"source":"无论是内置fit方法，还是自定义训练循环，从CPU切换成单GPU训练模型都是非常方便的，无需更改任何代码。当存在可用的GPU时，如果不特意指定device，tensorflow会自动优先选择使用GPU来创建张量和执行张量计算。\n\n但如果是在公司或者学校实验室的服务器环境，存在多个GPU和多个使用者时，为了不让单个同学的任务占用全部GPU资源导致其他同学无法使用（tensorflow默认获取全部GPU的全部内存资源权限，但实际上只使用一个GPU的部分资源），我们通常会在开头增加以下几行代码以控制每个任务使用的GPU编号和显存大小，以便其他同学也能够同时训练模型。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BD4C1DD23EB143B9835B3625CF41CC37","trusted":true,"mdEditEnable":true},"source":"注：以下代码需要在GPU机器环境下才能正确执行。\n\n可以在kesci选择GPU执行环境，也可以在Colab中使用GPU。\n\n在Colab笔记本中：修改->笔记本设置->硬件加速器 中选择 GPU\n\n可通过以下colab链接测试效果《tf_单GPU》：\n\nhttps://colab.research.google.com/drive/1r5dLoeJq5z01sU72BX2M5UiNSkuxsEFe"},{"cell_type":"code","execution_count":2,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7271D7768AFB4048BB5A731917797F2F","trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"2.3.0\n","name":"stdout"}],"source":"import tensorflow as tf\nprint(tf.__version__)"},{"cell_type":"code","execution_count":3,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A81955B9EF954EFB8B5CDCE0FBBE4B7E","trusted":true,"collapsed":false,"scrolled":false},"outputs":[],"source":"from tensorflow.keras import * \n\n#打印时间分割线\n@tf.function\ndef printbar():\n    ts = tf.timestamp()\n    today_ts = ts%(24*60*60)\n\n    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n    minite = tf.cast((today_ts%3600)//60,tf.int32)\n    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n    \n    def timeformat(m):\n        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n            return(tf.strings.format(\"0{}\",m))\n        else:\n            return(tf.strings.format(\"{}\",m))\n    \n    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n                timeformat(second)],separator = \":\")\n    tf.print(\"==========\"*8,end = \"\")\n    tf.print(timestring)\n    "},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EC9E850543074A14B1AE71CD3BAF1671","trusted":true},"source":"### 一，GPU设置"},{"cell_type":"code","execution_count":4,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0F1CD5D1776644D1AADF7507796AB7F4","trusted":true,"collapsed":false,"scrolled":false},"outputs":[],"source":"gpus = tf.config.list_physical_devices(\"GPU\")\n\nif gpus:\n    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n    tf.config.set_visible_devices([gpu0],\"GPU\") "},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6A116ABAC2234E889B6A54479A539EA1","trusted":true},"source":"比较GPU和CPU的计算速度"},{"cell_type":"code","execution_count":5,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FE04F670C68F496DB17DCED58B593451","trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================15:12:32\n2.24953778e+11\n================================================================================15:12:32\n","name":"stdout"}],"source":"printbar()\nwith tf.device(\"/gpu:0\"):\n    tf.random.set_seed(0)\n    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)\n    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)\n    c = a@b\n    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))\nprintbar()"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"82AA3E4761F4430B8F3B605EACFE409B","trusted":true},"source":"```\n================================================================================17:37:01\n2.24953778e+11\n================================================================================17:37:01\n```"},{"cell_type":"code","execution_count":6,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D12AEFD4A558461AB6B84728C0EA3F43","trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"================================================================================15:12:35\n2.24953778e+11\n================================================================================15:12:38\n","name":"stdout"}],"source":"printbar()\nwith tf.device(\"/cpu:0\"):\n    tf.random.set_seed(0)\n    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)\n    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)\n    c = a@b\n    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))\nprintbar()"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8816FB6895974B918696F7024EA13A76","trusted":true},"source":"```\n================================================================================17:37:34\n2.24953795e+11\n================================================================================17:37:40\n```"},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"602820AA1E1049D18835BC1AE7333F51","trusted":true},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F9035A00664A49AF81A4FCA562931AC3","trusted":true},"source":"### 二，准备数据"},{"cell_type":"code","execution_count":7,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C188D1CE481144D0BB3C7D29BC192E30","trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n2113536/2110848 [==============================] - 0s 0us/step\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n","name":"stderr"}],"source":"MAX_LEN = 300\nBATCH_SIZE = 32\n(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()\nx_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)\nx_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)\n\nMAX_WORDS = x_train.max()+1\nCAT_NUM = y_train.max()+1\n\nds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \\\n          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n   \nds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \\\n          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n          .prefetch(tf.data.experimental.AUTOTUNE).cache()\n          "},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E0077C17E5EF42DAB87A56AE7E61AA22","trusted":true},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"29258A29B58547C084BB7E63A06E653D","trusted":true},"source":"### 三，定义模型"},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8AB4AECB10954C4AB3150E1497E55AB8","trusted":true},"outputs":[],"source":"tf.keras.backend.clear_session()\n\ndef create_model():\n    \n    model = models.Sequential()\n\n    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n    model.add(layers.MaxPool1D(2))\n    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n    model.add(layers.MaxPool1D(2))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(CAT_NUM,activation = \"softmax\"))\n    return(model)\n\nmodel = create_model()\nmodel.summary()\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"360FFA98FB8649F88C83E37B20F09CCE","trusted":true},"source":"```\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 300, 7)            216874    \n_________________________________________________________________\nconv1d (Conv1D)              (None, 296, 64)           2304      \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 146, 32)           6176      \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2336)              0         \n_________________________________________________________________\ndense (Dense)                (None, 46)                107502    \n=================================================================\nTotal params: 332,856\nTrainable params: 332,856\nNon-trainable params: 0\n_________________________________________________________________\n```"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"847C69E4CD404E6DB792E4D260288478","mdEditEnable":false,"trusted":true},"source":"### 四，训练模型"},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"40F11D24E77843D0870D4CC07799671F","trusted":true},"outputs":[],"source":"optimizer = optimizers.Nadam()\nloss_func = losses.SparseCategoricalCrossentropy()\n\ntrain_loss = metrics.Mean(name='train_loss')\ntrain_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\nvalid_loss = metrics.Mean(name='valid_loss')\nvalid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n\n@tf.function\ndef train_step(model, features, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(features,training = True)\n        loss = loss_func(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    train_loss.update_state(loss)\n    train_metric.update_state(labels, predictions)\n    \n@tf.function\ndef valid_step(model, features, labels):\n    predictions = model(features)\n    batch_loss = loss_func(labels, predictions)\n    valid_loss.update_state(batch_loss)\n    valid_metric.update_state(labels, predictions)\n    \n\ndef train_model(model,ds_train,ds_valid,epochs):\n    for epoch in tf.range(1,epochs+1):\n        \n        for features, labels in ds_train:\n            train_step(model,features,labels)\n\n        for features, labels in ds_valid:\n            valid_step(model,features,labels)\n\n        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n        \n        if epoch%1 ==0:\n            printbar()\n            tf.print(tf.strings.format(logs,\n            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n            tf.print(\"\")\n            \n        train_loss.reset_states()\n        valid_loss.reset_states()\n        train_metric.reset_states()\n        valid_metric.reset_states()\n\ntrain_model(model,ds_train,ds_test,10)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5A1CD3AD4EB04D119A3AA2393455878F","mdEditEnable":false,"trusted":true},"source":"```\n================================================================================17:13:26\nEpoch=1,Loss:1.96735072,Accuracy:0.489200622,Valid Loss:1.64124215,Valid Accuracy:0.582813919\n\n================================================================================17:13:28\nEpoch=2,Loss:1.4640888,Accuracy:0.624805152,Valid Loss:1.5559175,Valid Accuracy:0.607747078\n\n================================================================================17:13:30\nEpoch=3,Loss:1.20681274,Accuracy:0.68581605,Valid Loss:1.58494771,Valid Accuracy:0.622439921\n\n================================================================================17:13:31\nEpoch=4,Loss:0.937500894,Accuracy:0.75361836,Valid Loss:1.77466083,Valid Accuracy:0.621994674\n\n================================================================================17:13:33\nEpoch=5,Loss:0.693960547,Accuracy:0.822199941,Valid Loss:2.00267363,Valid Accuracy:0.6197685\n\n================================================================================17:13:35\nEpoch=6,Loss:0.519614,Accuracy:0.870296121,Valid Loss:2.23463202,Valid Accuracy:0.613980412\n\n================================================================================17:13:37\nEpoch=7,Loss:0.408562034,Accuracy:0.901246965,Valid Loss:2.46969271,Valid Accuracy:0.612199485\n\n================================================================================17:13:39\nEpoch=8,Loss:0.339028627,Accuracy:0.920062363,Valid Loss:2.68585229,Valid Accuracy:0.615316093\n\n================================================================================17:13:41\nEpoch=9,Loss:0.293798745,Accuracy:0.92930305,Valid Loss:2.88995624,Valid Accuracy:0.613535166\n\n================================================================================17:13:43\nEpoch=10,Loss:0.263130337,Accuracy:0.936651051,Valid Loss:3.09705234,Valid Accuracy:0.612644672\n```"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}